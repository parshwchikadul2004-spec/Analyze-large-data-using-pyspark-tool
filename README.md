# Analyze-large-data-using-pyspark-tool
Scalability in PySpark is shown by repartitioning data for parallel processing and caching for better performance. Execution time of count, filter, and groupBy is measured, and the dataset is increased to show efficient handling of large data.
